# Awesome Copyright in Generative AI [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re)

A curated list of awesome works in Copyright Protection for Deep Generative Models.

### Part 1. Image generative models. 
Including [Autoencoders](https://arxiv.org/abs/1312.6114), [GAN](https://arxiv.org/abs/1406.2661) and [Diffusion models](https://arxiv.org/abs/2006.11239).
#### Source Data Owner Copyright Protection

##### Unrecognizable Examples
<details>

<summary> UnGANable: Defending Against GAN-based Face Manipulation </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2210.00957)

&nbsp;&nbsp;&nbsp;Against GAN Inversion; create adversarial examples to maximize the discrepancy between adversarial examples and original samples in the latent space of the generator.

</details>

<details>

<summary> Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2003.01279)

&nbsp;&nbsp;&nbsp;Against Image-translation GAN; create adversarial examples for the generator.

</details>

<details>

<summary> Disrupting Image-Translation-Based DeepFake Algorithms with Adversarial Attacks </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://openaccess.thecvf.com/content_WACVW_2020/papers/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.pdf)

&nbsp;&nbsp;&nbsp;Against DeepNude; defining Nullifying Attack and Distorting Attack. 

</details>

<details>

<summary> Initiative defense against facial manipulation </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16254)

&nbsp;&nbsp;&nbsp;Grey-box and black-box adversarial examples.

</details>

<details>

<summary> Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2302.04578)

&nbsp;&nbsp;&nbsp;Adversarial examples generated via training loss of Diffusion models.

</details>

<details>

<summary> Raising the cost of malicious ai-powered image editing </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2302.04578)

&nbsp;&nbsp;&nbsp; Encoder attack and Diffusion attack.

</details>

<details>

<summary> Defending against gan-based deepfake attacks via transformation-aware ad- versarial faces </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2006.07421)

&nbsp;&nbsp;&nbsp; Utilize adversarial examples against Deepfake models in training.

</details>

<details>

<summary> Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2206.00477)

&nbsp;&nbsp;&nbsp; They observed that adversarial perturbations on the Lab color space are robust to input reconstruction. Therefore, they converted the input from RGB space to the Lab color space and added perceptual-aware adversarial perturbations to the color channel to maintain robustness against input transformations.

</details>

<details>

<summary> Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2302.04222)

&nbsp;&nbsp;&nbsp; The core idea of GLAZE is to guide the diffusion model to learn an alternative target style T that is totally different from the style of protected images.

</details>

<details>

<summary> Mist: Towards improved adversarial examples for diffusion models. </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2305.12683)

&nbsp;&nbsp;&nbsp; Improve the transferability of adversarial examples by combining different losses.

</details>

<details>

<summary> Anti-dreambooth: Protecting users from personalized text-to-image synthesis. </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2303.15433)

&nbsp;&nbsp;&nbsp; Against DreamBooth and generate poisons via a bi-level optimization problem.

</details>

<details>

<summary> Towards prompt-robust face privacy protection via adversarial decoupling augmentation framework.  </summary>

&nbsp;&nbsp;&nbsp;[[paper]](https://arxiv.org/abs/2305.03980)

&nbsp;&nbsp;&nbsp; Introduces multi-level text-related augmentations for defense stability against various attacker prompts

</details>






##### Watermarks

##### Unlearning

#### Model Copyright Protection

##### Parameter-based

##### Image-based

##### Trigger-based

### Part 2. Text generative models.
Specifically on Large Language Models (LLMs).

#### Data Copyright Protection

#### Model Copyright Protection

### Other domains.

#### Code generation

#### Graph generation

#### Video generation

#### Audio generation


